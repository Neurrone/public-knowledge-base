<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><link rel="icon" href="/favicon.ico"/><title>Local</title><meta name="robots" content="index,follow"/><meta name="googlebot" content="index,follow"/><meta name="description" content="Personal knowledge Base"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:site" content="neurrone"/><meta name="twitter:creator" content="neurrone"/><meta property="og:title" content="Local"/><meta property="og:description" content="Personal knowledge Base"/><meta property="og:url" content="https://kb.neurrone.com/notes/9u86t3vdwdylduprhqd0bgz/"/><meta property="og:type" content="article"/><meta property="article:published_time" content="2/18/2024"/><meta property="article:modified_time" content="3/16/2024"/><link rel="canonical" href="https://kb.neurrone.com/notes/9u86t3vdwdylduprhqd0bgz/"/><meta name="next-head-count" content="17"/><link rel="preload" href="/_next/static/css/8e7b7e4bce421c0a.css" as="style"/><link rel="stylesheet" href="/_next/static/css/8e7b7e4bce421c0a.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-3d209faeb64f2f97.js" defer=""></script><script src="/_next/static/chunks/framework-28c999baf2863c3d.js" defer=""></script><script src="/_next/static/chunks/main-104451f3d1a5c4bc.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fc92abe5e8d10d3.js" defer=""></script><script src="/_next/static/chunks/935-4dee79e80b8641c6.js" defer=""></script><script src="/_next/static/chunks/6-50972def09142ee2.js" defer=""></script><script src="/_next/static/chunks/pages/notes/%5Bid%5D-78d472fa3b924116.js" defer=""></script><script src="/_next/static/r_rF6tWKMbpd0SFr3zQ3k/_buildManifest.js" defer=""></script><script src="/_next/static/r_rF6tWKMbpd0SFr3zQ3k/_ssgManifest.js" defer=""></script></head><body><div id="__next" data-reactroot=""><section class="ant-layout" style="width:100%;min-height:100%"><header class="ant-layout-header" style="position:fixed;isolation:isolate;z-index:1;width:100%;border-bottom:1px solid #d4dadf;height:64px;padding:0 24px 0 2px"><div class="ant-row ant-row-center" style="max-width:992px;justify-content:space-between;margin:0 auto"><div style="display:flex" class="ant-col ant-col-xs-20 ant-col-sm-4"></div><div class="ant-col gutter-row ant-col-xs-0 ant-col-sm-20 ant-col-md-20 ant-col-lg-19"><div class="ant-select ant-select-lg ant-select-auto-complete ant-select-single ant-select-allow-clear ant-select-show-search" style="width:100%"><div class="ant-select-selector"><span class="ant-select-selection-search"><input type="search" autoComplete="off" class="ant-select-selection-search-input" role="combobox" aria-haspopup="listbox" aria-owns="undefined_list" aria-autocomplete="list" aria-controls="undefined_list" aria-activedescendant="undefined_list_0" value=""/></span><span class="ant-select-selection-placeholder">For full text search please use the &#x27;?&#x27; prefix. e.g. ? Onboarding</span></div></div></div><div style="display:none;align-items:center;justify-content:center" class="ant-col ant-col-xs-4 ant-col-sm-4 ant-col-md-0 ant-col-lg-0"><span role="img" aria-label="menu" style="font-size:24px" tabindex="-1" class="anticon anticon-menu"><svg viewBox="64 64 896 896" focusable="false" data-icon="menu" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M904 160H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0 624H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0-312H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8z"></path></svg></span></div></div></header><section class="ant-layout" style="margin-top:64px;display:flex;flex-direction:row"><div class="site-layout-sidebar" style="flex:0 0 auto;width:calc(max((100% - 992px) / 2, 0px) + 200px);min-width:200px;padding-left:calc((100% - 992px) / 2)"><aside class="ant-layout-sider ant-layout-sider-dark" style="position:fixed;overflow:auto;height:calc(100vh - 64px);background-color:transparent;flex:0 0 200px;max-width:200px;min-width:200px;width:200px"><div class="ant-layout-sider-children"></div></aside></div><main class="ant-layout-content side-layout-main" style="max-width:1200px;min-width:0;display:block"><div style="padding:0 24px"><div class="main-content" role="main"><div class="ant-row"><div class="ant-col ant-col-24"><div class="ant-row" style="margin-left:-10px;margin-right:-10px"><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-24 ant-col-md-18"><div><h1 id="running-llms-locally">Running LLMs Locally<a aria-hidden="true" class="anchor-heading icon-link" href="#running-llms-locally"></a></h1>
<h2 id="local-inference">Local Inference<a aria-hidden="true" class="anchor-heading icon-link" href="#local-inference"></a></h2>
<p>These are wrappers over llama.cpp</p>
<ul>
<li><a href="https://simonwillison.net/2023/Nov/29/llamafile/">Lambdafile</a></li>
<li><a href="https://ollama.com/blog/windows-preview">Ollama</a></li>
<li><a href="https://github.com/LlamaEdge/LlamaEdge">LlamaEdge</a>: running models via WebAssembly</li>
</ul>
<h3 id="compiling-llamacpp">Compiling llama.cpp<a aria-hidden="true" class="anchor-heading icon-link" href="#compiling-llamacpp"></a></h3>
<h4 id="with-vulkan">With Vulkan<a aria-hidden="true" class="anchor-heading icon-link" href="#with-vulkan"></a></h4>
<p>Install the Vulkan SDK.</p>
<pre class="language-sh"><code class="language-sh">set VULKAN_SDK=C:\VulkanSDK\1.3.275.0
mkdir build
cd build
cmake .. --fresh -DLLAMA_AVX512=on -DLLAMA_AVX512_VBMI=on -DLLAMA_AVX512_VNNI=on -DLLAMA_VULKAN=on
cmake --build . --config Release
</code></pre>
<p>Test performance by:</p>
<pre class="language-sh"><code class="language-sh">main.exe -m "d:/downloads/llama-2-7b-chat.Q4_K_M.gguf" -p "Hi you how are you" -ngl 33
./llama-bench -m "d:/downloads/llama-2-7b-chat.Q4_K_M.gguf" -p 3968
</code></pre>
<p>Experiment: trying to get AOCL support working but it doesn't compile:</p>
<pre class="language-sh"><code class="language-sh">-DLLAMA_BLAS_VENDOR=AOCL-mt -DLLAMA_BLAS=ON -DBLAS_LIBRARIES="C:\Program Files\AMD\AOCL-Windows\amd-blis\lib\LP64\AOCL-LibBlis-Win-dll.dll" -DBLAS_INCLUDE_DIRS="C:\Program Files\AMD\AOCL-Windows\amd-blis\include\LP64"
</code></pre>
<h4 id="rocm">ROCm<a aria-hidden="true" class="anchor-heading icon-link" href="#rocm"></a></h4>
<p>Didn't manage to get this to work, since AMD doesn't support the 7840U yet with ROCm.</p>
<p>Install visual studio C++ workload, and select the Clang tools.</p>
<p>Install ROCM (unchecking the visual studio extension as that causes an error), and Pyrl as well.</p>
<pre class="language-sh"><code class="language-sh">cmake.exe .. --fresh -G "Ninja" -DCMAKE_BUILD_TYPE=Release -DLLAMA_HIPBLAS=on -DLLAMA_AVX512_VBMI=on -DLLAMA_AVX512_VNNI=on -DCMAKE_C_COMPILER="clang.exe" -DCMAKE_CXX_COMPILER="clang++.exe" -DAMDGPU_TARGETS="gfx1100" -DCMAKE_PREFIX_PATH="C:\Program Files\AMD\ROCm\5.7"

cmake --build . --config Release
</code></pre>
<p>Trying to run it results in an error saying that the tensor for GFX1103 isn't found. The commonly suggested solution to set <code>HSA_OVERRIDE_GFX_VERSION=11.0.0</code> only works on Linux.</p>
<p>Other relevant links:</p>
<ul>
<li><a href="https://llm-tracker.info/howto/AMD-GPUs">Notes on AMD GPUs</a></li>
<li><a href="https://www.reddit.com/r/LocalLLaMA/comments/18ny92b/full_memory_available_for_amd_apus/">UMA support</a></li>
</ul>
<h2 id="frontends">Frontends<a aria-hidden="true" class="anchor-heading icon-link" href="#frontends"></a></h2>
<ul>
<li><a href="https://github.com/open-webui/open-webui">open-webui</a></li>
<li><a href="https://github.com/codeofdusk/gptcmd">gptcmd</a></li>
<li><a href="https://pypi.org/project/llm/">LLM Python package</a></li>
<li><a href="https://github.com/chigkim/VOLlama/">VOLlama</a></li>
</ul>
<h2 id="models">Models<a aria-hidden="true" class="anchor-heading icon-link" href="#models"></a></h2>
<ul>
<li><a href="https://ollama.com/blog/run-llama2-uncensored-locally">Run Llama 2 uncensored locally</a></li>
<li><a href="https://github.com/LAION-AI/Open-Assistant?tab=readme-ov-file">OpenAssistant</a></li>
<li><a href="https://ai.google.dev/gemma">Gemma models from Google</a></li>
</ul>
<h2 id="related">Related<a aria-hidden="true" class="anchor-heading icon-link" href="#related"></a></h2>
<ul>
<li><a href="https://github.com/janhq/awesome-local-ai">awesome-local-ai</a></li>
<li><a href="https://github.com/vince-lam/awesome-local-llms">Awesome Local LLMs</a></li>
<li><a href="https://news.ycombinator.com/item?id=39307330">HN thread on OpenAPI compatibility</a></li>
</ul></div></div><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-0 ant-col-md-6"><div><div class=""><div class="ant-anchor-wrapper dendron-toc" style="max-height:calc(100vh - 64px);z-index:1"><div class="ant-anchor"><div class="ant-anchor-ink"><span class="ant-anchor-ink-ball"></span></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#running-llms-locally" title="Running LLMs Locally">Running LLMs Locally</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#local-inference" title="Local Inference">Local Inference</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#compiling-llamacpp" title="Compiling llama.cpp">Compiling llama.cpp</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#with-vulkan" title="With Vulkan">With Vulkan</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#rocm" title="ROCm">ROCm</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#frontends" title="Frontends">Frontends</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#models" title="Models">Models</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#related" title="Related">Related</a></div></div></div></div></div></div></div></div></div></div></div><div class="ant-divider ant-divider-horizontal" role="separator"></div><footer class="ant-layout-footer" style="padding:0 24px 24px"></footer></main></section></section></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"note":{"id":"9u86t3vdwdylduprhqd0bgz","title":"Local","desc":"","updated":1710573390216,"created":1708269900687,"custom":{},"fname":"dev.llm.local","type":"note","vault":{"name":"main","fsPath":"main","useFMTitle":false},"contentHash":"fd9ced96f3eb55967da60f9bd920ba8c","links":[],"anchors":{"running-llms-locally":{"type":"header","text":"Running LLMs Locally","value":"running-llms-locally","line":8,"column":0,"depth":1},"local-inference":{"type":"header","text":"Local Inference","value":"local-inference","line":10,"column":0,"depth":2},"compiling-llamacpp":{"type":"header","text":"Compiling llama.cpp","value":"compiling-llamacpp","line":18,"column":0,"depth":3},"with-vulkan":{"type":"header","text":"With Vulkan","value":"with-vulkan","line":20,"column":0,"depth":4},"rocm":{"type":"header","text":"ROCm","value":"rocm","line":45,"column":0,"depth":4},"frontends":{"type":"header","text":"Frontends","value":"frontends","line":66,"column":0,"depth":2},"models":{"type":"header","text":"Models","value":"models","line":73,"column":0,"depth":2},"related":{"type":"header","text":"Related","value":"related","line":79,"column":0,"depth":2}},"children":[],"parent":"tt62xbuvkdvr6jnnxzzbvpm","data":{}},"body":"\u003ch1 id=\"running-llms-locally\"\u003eRunning LLMs Locally\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#running-llms-locally\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003ch2 id=\"local-inference\"\u003eLocal Inference\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#local-inference\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eThese are wrappers over llama.cpp\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://simonwillison.net/2023/Nov/29/llamafile/\"\u003eLambdafile\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://ollama.com/blog/windows-preview\"\u003eOllama\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/LlamaEdge/LlamaEdge\"\u003eLlamaEdge\u003c/a\u003e: running models via WebAssembly\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"compiling-llamacpp\"\u003eCompiling llama.cpp\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#compiling-llamacpp\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003ch4 id=\"with-vulkan\"\u003eWith Vulkan\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#with-vulkan\"\u003e\u003c/a\u003e\u003c/h4\u003e\n\u003cp\u003eInstall the Vulkan SDK.\u003c/p\u003e\n\u003cpre class=\"language-sh\"\u003e\u003ccode class=\"language-sh\"\u003eset VULKAN_SDK=C:\\VulkanSDK\\1.3.275.0\nmkdir build\ncd build\ncmake .. --fresh -DLLAMA_AVX512=on -DLLAMA_AVX512_VBMI=on -DLLAMA_AVX512_VNNI=on -DLLAMA_VULKAN=on\ncmake --build . --config Release\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTest performance by:\u003c/p\u003e\n\u003cpre class=\"language-sh\"\u003e\u003ccode class=\"language-sh\"\u003emain.exe -m \"d:/downloads/llama-2-7b-chat.Q4_K_M.gguf\" -p \"Hi you how are you\" -ngl 33\n./llama-bench -m \"d:/downloads/llama-2-7b-chat.Q4_K_M.gguf\" -p 3968\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eExperiment: trying to get AOCL support working but it doesn't compile:\u003c/p\u003e\n\u003cpre class=\"language-sh\"\u003e\u003ccode class=\"language-sh\"\u003e-DLLAMA_BLAS_VENDOR=AOCL-mt -DLLAMA_BLAS=ON -DBLAS_LIBRARIES=\"C:\\Program Files\\AMD\\AOCL-Windows\\amd-blis\\lib\\LP64\\AOCL-LibBlis-Win-dll.dll\" -DBLAS_INCLUDE_DIRS=\"C:\\Program Files\\AMD\\AOCL-Windows\\amd-blis\\include\\LP64\"\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4 id=\"rocm\"\u003eROCm\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#rocm\"\u003e\u003c/a\u003e\u003c/h4\u003e\n\u003cp\u003eDidn't manage to get this to work, since AMD doesn't support the 7840U yet with ROCm.\u003c/p\u003e\n\u003cp\u003eInstall visual studio C++ workload, and select the Clang tools.\u003c/p\u003e\n\u003cp\u003eInstall ROCM (unchecking the visual studio extension as that causes an error), and Pyrl as well.\u003c/p\u003e\n\u003cpre class=\"language-sh\"\u003e\u003ccode class=\"language-sh\"\u003ecmake.exe .. --fresh -G \"Ninja\" -DCMAKE_BUILD_TYPE=Release -DLLAMA_HIPBLAS=on -DLLAMA_AVX512_VBMI=on -DLLAMA_AVX512_VNNI=on -DCMAKE_C_COMPILER=\"clang.exe\" -DCMAKE_CXX_COMPILER=\"clang++.exe\" -DAMDGPU_TARGETS=\"gfx1100\" -DCMAKE_PREFIX_PATH=\"C:\\Program Files\\AMD\\ROCm\\5.7\"\n\ncmake --build . --config Release\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTrying to run it results in an error saying that the tensor for GFX1103 isn't found. The commonly suggested solution to set \u003ccode\u003eHSA_OVERRIDE_GFX_VERSION=11.0.0\u003c/code\u003e only works on Linux.\u003c/p\u003e\n\u003cp\u003eOther relevant links:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://llm-tracker.info/howto/AMD-GPUs\"\u003eNotes on AMD GPUs\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.reddit.com/r/LocalLLaMA/comments/18ny92b/full_memory_available_for_amd_apus/\"\u003eUMA support\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"frontends\"\u003eFrontends\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#frontends\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/open-webui/open-webui\"\u003eopen-webui\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/codeofdusk/gptcmd\"\u003egptcmd\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://pypi.org/project/llm/\"\u003eLLM Python package\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/chigkim/VOLlama/\"\u003eVOLlama\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"models\"\u003eModels\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#models\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://ollama.com/blog/run-llama2-uncensored-locally\"\u003eRun Llama 2 uncensored locally\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/LAION-AI/Open-Assistant?tab=readme-ov-file\"\u003eOpenAssistant\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://ai.google.dev/gemma\"\u003eGemma models from Google\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"related\"\u003eRelated\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#related\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/janhq/awesome-local-ai\"\u003eawesome-local-ai\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/vince-lam/awesome-local-llms\"\u003eAwesome Local LLMs\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://news.ycombinator.com/item?id=39307330\"\u003eHN thread on OpenAPI compatibility\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e","noteIndex":{"id":"root","title":"root","desc":"","updated":1622102326440,"created":1595961348801,"custom":{"stub":false,"nav_order":0,"permalink":"/"},"fname":"root","type":"note","vault":{"name":"main","fsPath":"main","useFMTitle":false},"contentHash":"2222728f026e86e75d06a6048c4f43ed","links":[],"anchors":{"personal-knowledge-base":{"type":"header","text":"Personal Knowledge Base","value":"personal-knowledge-base","line":8,"column":0,"depth":1}},"children":["9fa47ef8-a44b-4e36-929f-4aa7835b474d","6e43ea37-b3e7-44cf-aac7-31ae947d32a2","a10a1802-11d0-4557-a374-89712109cf90","0b9beaed-19ee-4124-a880-07e520da2b17","ddf3e9b2-ea12-4129-8785-06d6b82d4fcb","f3vpxyah3zaq9t6z9a0j3lp","amek1xrre7mnb2h0lrdydx2","f3yyev03tucxssw2gzv1wcq","1j2fadhuewmgicemldm8r98","b4be93a8-69e3-4427-b4cd-53593c3ab387","5bbec2d1-4abe-4a2a-8bc5-5f3d4e229766","4a064a97-3d2d-4429-9a08-5b158dc7ef35","543e830d-feac-4048-8075-58c406bc1105","8875d2cf-85f8-41e8-b16d-c3707e40d74e","4k2ot7nwcxbum3rwhovp8bp","8ec37b11-59ad-4ea8-82d9-3186e7e0cd29","dOehc8Yap1zIbEqLvQvOt","i0g2e5s84q33i4o88s0zzcb","b58269ed-9a0e-4213-b5b0-6d59c81d9899","b4fc6d48-17ae-4764-acdb-ec0f52707860","091b67bf-8bfa-454c-bbd3-7d84d8216229","f6fbee87-be27-471c-99bd-4af45d6a3156","fb9e4130-eaf8-425d-9039-5bc2ee876f18","4fb536a9-c146-4df3-8f30-9b2fb52a77f7","trsef01c5ekr6t8wr5nkh4w","2vt9pzj3gj1435jfgj74k4n","21ef758d-ca84-4be0-8eed-5e95ab3b4a7e","5fb5baa9-8b3d-4044-9c03-bbabfbdfe78a","2f6af89e-df82-4144-87fc-a2be66a0e657","wy23hkmakngujlepw9j96ea","POgyWcSIqi29tpqfP5uyS","525uy1b41n5r1x0n5oi5sio","wejy4xa1wsjl9g8pvp2cj73","cesi12nkdgz59ybtsqahxlz","p5fjnofwee5xr2fnd959i4w","ratxuio8f3tpq39ykqxby15","s7rnvuyzhqdygvmfgvjha9j","ut9i6vl0lfslzfmbk0serwq","e8sg85f9dbisa2nby57cfit","4q4k68xiq7t548i1ayjqzih","xqldazl2zsiop6n85eoq389"],"parent":null,"data":{},"body":"\n# Personal Knowledge Base\n\nHi, and welcome to [Dickson Tan's](https://neurrone.com/pages/about/) personal knowledge base.\n\nThis is an experiment in note taking with the Zettelkasten method. This site is rendered with [Dendron](https://wiki.dendron.so/) which generates backlinks and an index, and provides other convenience features.\n"},"collectionChildren":null,"customHeadContent":null,"config":{"version":5,"dev":{"previewV2Enabled":false,"enablePreviewV2":true},"commands":{"lookup":{"note":{"selectionMode":"extract","confirmVaultOnCreate":false,"leaveTrace":false,"bubbleUpCreateNew":true,"fuzzThreshold":0.2,"vaultSelectionModeOnCreate":"smart"}},"insertNote":{"initialValue":"templates"},"insertNoteLink":{"aliasMode":"none","enableMultiSelect":false},"insertNoteIndex":{"enableMarker":false},"randomNote":{},"copyNoteLink":{"aliasMode":"title"},"templateHierarchy":"template"},"workspace":{"dendronVersion":"0.85.0","vaults":[{"fsPath":"human-dynamics","name":"Human Dynamics","useFMTitle":false},{"name":"govtech","fsPath":"govtech","visibility":"private","useFMTitle":false},{"name":"health","fsPath":"health","useFMTitle":false},{"name":"main","fsPath":"main","useFMTitle":false},{"name":"personal","fsPath":"personal","visibility":"private"}],"journal":{"dailyDomain":"daily","name":"journal","dateFormat":"y.MM.dd","addBehavior":"childOfDomain"},"scratch":{"name":"scratch","dateFormat":"y.MM.dd.HHmmss","addBehavior":"asOwnDomain"},"graph":{"zoomSpeed":1,"createStub":false},"disableTelemetry":true,"enableAutoCreateOnDefinition":false,"enableXVaultWikiLink":false,"enableRemoteVaultInit":true,"workspaceVaultSyncMode":"noCommit","enableAutoFoldFrontmatter":false,"maxPreviewsCached":10,"maxNoteLength":204800,"enableUserTags":true,"enableHashTags":true,"task":{"name":"","dateFormat":"","addBehavior":"childOfCurrent","statusSymbols":{"":" ","wip":"w","done":"x","assigned":"a","moved":"m","blocked":"b","delegated":"l","dropped":"d","pending":"y"},"prioritySymbols":{"H":"high","M":"medium","L":"low"},"todoIntegration":false,"createTaskSelectionType":"selection2link","taskCompleteStatus":["done","x"]},"enableEditorDecorations":true,"enableFullHierarchyNoteTitle":false},"preview":{"enableFMTitle":false,"enableNoteTitleForLink":true,"enablePrettyRefs":true,"enableKatex":true,"enableFrontmatterTags":true,"enableHashesForFMTags":false,"automaticallyShowPreview":false},"publishing":{"enableFMTitle":false,"enableNoteTitleForLink":true,"enablePrettyRefs":true,"enableKatex":true,"copyAssets":true,"siteHierarchies":["root"],"writeStubs":false,"siteRootDir":"docs","seo":{"title":"Personal Knowledge Base","description":"Personal knowledge Base","author":"Dickson Tan","twitter":"neurrone"},"github":{"enableEditLink":true,"editLinkText":"Edit this page on GitHub","editBranch":"main","editViewMode":"tree"},"enableSiteLastModified":true,"enableFrontmatterTags":true,"enableHashesForFMTags":false,"enableRandomlyColoredTags":true,"enableTaskNotes":true,"enablePrettyLinks":true,"searchMode":"lookup","siteUrl":"https://kb.neurrone.com","duplicateNoteBehavior":{"action":"useVault","payload":["main","health","Human Dynamics"]},"siteFaviconPath":"favicon.ico","siteIndex":"root"}}},"__N_SSG":true},"page":"/notes/[id]","query":{"id":"9u86t3vdwdylduprhqd0bgz"},"buildId":"r_rF6tWKMbpd0SFr3zQ3k","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>