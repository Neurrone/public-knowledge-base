<h1 id="running-llms-locally">Running LLMs Locally<a aria-hidden="true" class="anchor-heading icon-link" href="#running-llms-locally"></a></h1>
<h2 id="local-inference">Local Inference<a aria-hidden="true" class="anchor-heading icon-link" href="#local-inference"></a></h2>
<p>These are wrappers over llama.cpp</p>
<ul>
<li><a href="https://simonwillison.net/2023/Nov/29/llamafile/">Lambdafile</a></li>
<li><a href="https://ollama.com/blog/windows-preview">Ollama</a></li>
<li><a href="https://github.com/LlamaEdge/LlamaEdge">LlamaEdge</a>: running models via WebAssembly</li>
</ul>
<h2 id="frontends">Frontends<a aria-hidden="true" class="anchor-heading icon-link" href="#frontends"></a></h2>
<ul>
<li><a href="https://github.com/open-webui/open-webui">open-webui</a></li>
<li><a href="https://github.com/codeofdusk/gptcmd">gptcmd</a></li>
<li><a href="https://pypi.org/project/llm/">LLM Python package</a></li>
</ul>
<h2 id="models">Models<a aria-hidden="true" class="anchor-heading icon-link" href="#models"></a></h2>
<ul>
<li><a href="https://ollama.com/blog/run-llama2-uncensored-locally">Run Llama 2 uncensored locally</a></li>
<li><a href="https://github.com/LAION-AI/Open-Assistant?tab=readme-ov-file">OpenAssistant</a></li>
<li><a href="https://ai.google.dev/gemma">Gemma models from Google</a></li>
</ul>
<h2 id="related">Related<a aria-hidden="true" class="anchor-heading icon-link" href="#related"></a></h2>
<ul>
<li><a href="https://github.com/janhq/awesome-local-ai">awesome-local-ai</a></li>
<li><a href="https://news.ycombinator.com/item?id=39307330">HN thread on OpenAPI compatibility</a></li>
</ul>