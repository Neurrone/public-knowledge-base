
# Running LLMs Locally

## Local Inference

These are wrappers over llama.cpp

- [Lambdafile](https://simonwillison.net/2023/Nov/29/llamafile/)
- [Ollama](https://ollama.com/blog/windows-preview)
- [LlamaEdge](https://github.com/LlamaEdge/LlamaEdge): running models via WebAssembly

## Frontends

- [open-webui](https://github.com/open-webui/open-webui)
- [gptcmd](https://github.com/codeofdusk/gptcmd)
- [LLM Python package](https://pypi.org/project/llm/)

## Models

- [Run Llama 2 uncensored locally](https://ollama.com/blog/run-llama2-uncensored-locally)
- [OpenAssistant](https://github.com/LAION-AI/Open-Assistant?tab=readme-ov-file)
- [Gemma models from Google](https://ai.google.dev/gemma)

## Related

- [awesome-local-ai](https://github.com/janhq/awesome-local-ai)
- [HN thread on OpenAPI compatibility](https://news.ycombinator.com/item?id=39307330)
